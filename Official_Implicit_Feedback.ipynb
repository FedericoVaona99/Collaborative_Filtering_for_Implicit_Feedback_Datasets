{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74a05ef",
   "metadata": {},
   "source": [
    "# Collaborative Filtering for Implicit Feedback (Last.fm 500)\n",
    "\n",
    "**Goal.** Re-implementation of *Implicit ALS* (Hu, Koren, Volinsky, 2008) on a reduced Last.fm dataset (~500 users), evaluating **novelty recovery** (re-watches removed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e1ab9d",
   "metadata": {},
   "source": [
    "## 0. Import Libraries\n",
    "This cell imports the required Python libraries for data manipulation and numerical computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73d6a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup & Imports ===\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# SciPy is used only for sparse matrices (CSR/CSC)\n",
    "from scipy import sparse\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "RNG_SEED = 24\n",
    "np.random.seed(RNG_SEED)\n",
    "random.seed(24)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"24\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fa8be9",
   "metadata": {},
   "source": [
    "# 1. Load and Clean Last.fm Datasets\n",
    "\n",
    "In this step we load the reduced Last.fm datasets:\n",
    "\n",
    "For reproducibility and runtime feasibility we restrict the dataset to the first 500 users. This reduction may affect optimal hyperparameters compared to the original paper.\n",
    "\n",
    "- **Profiles**: user demographic information (gender, age, country, signup date).  \n",
    "- **Listens**: implicit feedback records (user listening history).\n",
    "\n",
    "We produce two clean DataFrames: profiles and listens, ready for analysis, applying some data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bb5374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>country</th>\n",
       "      <th>signup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#id</td>\n",
       "      <td>gender</td>\n",
       "      <td>age</td>\n",
       "      <td>country</td>\n",
       "      <td>registered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Aug 13, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_000002</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Peru</td>\n",
       "      <td>Feb 24, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_000003</td>\n",
       "      <td>m</td>\n",
       "      <td>22</td>\n",
       "      <td>United States</td>\n",
       "      <td>Oct 30, 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_000004</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr 26, 2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  gender  age        country        signup\n",
       "0          #id  gender  age        country    registered\n",
       "1  user_000001       m  NaN          Japan  Aug 13, 2006\n",
       "2  user_000002       f  NaN           Peru  Feb 24, 2006\n",
       "3  user_000003       m   22  United States  Oct 30, 2005\n",
       "4  user_000004       f  NaN            NaN  Apr 26, 2006"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04T23:08:57Z</td>\n",
       "      <td>f1b1cf71-bd35-4e99-8624-24a6e15f133a</td>\n",
       "      <td>Deep Dish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck Me Im Famous (Pacha Ibiza)-09-28-2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04T13:54:10Z</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Composition 0919 (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04T13:52:04Z</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mc2 (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04T13:42:52Z</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hibari (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04T13:42:11Z</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mc1 (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id             timestamp                             artist_id  \\\n",
       "0  user_000001  2009-05-04T23:08:57Z  f1b1cf71-bd35-4e99-8624-24a6e15f133a   \n",
       "1  user_000001  2009-05-04T13:54:10Z  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8   \n",
       "2  user_000001  2009-05-04T13:52:04Z  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8   \n",
       "3  user_000001  2009-05-04T13:42:52Z  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8   \n",
       "4  user_000001  2009-05-04T13:42:11Z  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8   \n",
       "\n",
       "  artist_name track_id                                  track_name  \n",
       "0   Deep Dish      NaN  Fuck Me Im Famous (Pacha Ibiza)-09-28-2007  \n",
       "1        坂本龍一      NaN           Composition 0919 (Live_2009_4_15)  \n",
       "2        坂本龍一      NaN                        Mc2 (Live_2009_4_15)  \n",
       "3        坂本龍一      NaN                     Hibari (Live_2009_4_15)  \n",
       "4        坂本龍一      NaN                        Mc1 (Live_2009_4_15)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Load CSVs (profiles + listens) ===\n",
    "\n",
    "PROFILES_CSV = Path(\"Datasets-lastfm-reduced/profiles_first500.csv\")\n",
    "LISTENS_CSV  = Path(\"Datasets-lastfm-reduced/listens_first500_CAP.csv\")\n",
    "\n",
    "profiles = pd.read_csv(PROFILES_CSV)\n",
    "listens  = pd.read_csv(LISTENS_CSV)\n",
    "\n",
    "display(profiles.head())\n",
    "display(listens.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4029ee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiles shape: (499, 5)\n",
      "Listens shape: (1986196, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>country</th>\n",
       "      <th>signup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Aug 13, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_000002</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Peru</td>\n",
       "      <td>Feb 24, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_000003</td>\n",
       "      <td>m</td>\n",
       "      <td>22.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Oct 30, 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_000004</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr 26, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_000005</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>Jun 29, 2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id gender   age        country        signup\n",
       "0  user_000001      m   NaN          Japan  Aug 13, 2006\n",
       "1  user_000002      f   NaN           Peru  Feb 24, 2006\n",
       "2  user_000003      m  22.0  United States  Oct 30, 2005\n",
       "3  user_000004      f   NaN            NaN  Apr 26, 2006\n",
       "4  user_000005      m   NaN       Bulgaria  Jun 29, 2006"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_name</th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 23:08:57+00:00</td>\n",
       "      <td>f1b1cf71-bd35-4e99-8624-24a6e15f133a</td>\n",
       "      <td>Deep Dish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck Me Im Famous (Pacha Ibiza)-09-28-2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:54:10+00:00</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Composition 0919 (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:52:04+00:00</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mc2 (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:42:52+00:00</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hibari (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:42:11+00:00</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mc1 (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id                 timestamp  \\\n",
       "0  user_000001 2009-05-04 23:08:57+00:00   \n",
       "1  user_000001 2009-05-04 13:54:10+00:00   \n",
       "2  user_000001 2009-05-04 13:52:04+00:00   \n",
       "3  user_000001 2009-05-04 13:42:52+00:00   \n",
       "4  user_000001 2009-05-04 13:42:11+00:00   \n",
       "\n",
       "                                item_id  item_name track_id  \\\n",
       "0  f1b1cf71-bd35-4e99-8624-24a6e15f133a  Deep Dish      NaN   \n",
       "1  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8       坂本龍一      NaN   \n",
       "2  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8       坂本龍一      NaN   \n",
       "3  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8       坂本龍一      NaN   \n",
       "4  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8       坂本龍一      NaN   \n",
       "\n",
       "                                   track_name  \n",
       "0  Fuck Me Im Famous (Pacha Ibiza)-09-28-2007  \n",
       "1           Composition 0919 (Live_2009_4_15)  \n",
       "2                        Mc2 (Live_2009_4_15)  \n",
       "3                     Hibari (Live_2009_4_15)  \n",
       "4                        Mc1 (Live_2009_4_15)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Clean & preprocess data ===\n",
    "# Clean profiles: skips first row that repeats header names\n",
    "profiles = pd.read_csv(PROFILES_CSV,header=1)\n",
    "\n",
    "# Ensure timestamp is parsed as UTC\n",
    "listens[\"timestamp\"] = pd.to_datetime(listens[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "#rename columns for consistency\n",
    "profiles = profiles.rename(columns={\"#id\": \"user_id\", \"registered\": \"signup\"}) \n",
    "\n",
    "# We work at ARTIST level to avoid NaNs in track_id\n",
    "listens = listens.rename(columns={\"artist_id\": \"item_id\", \"artist_name\": \"item_name\"})\n",
    "# Drop rows with NaN item_id\n",
    "listens = listens[listens[\"item_id\"].notna()].copy() \n",
    "listens = listens[listens[\"timestamp\"] <= \"2009-06-20\"].copy()  # removed outlier value (2010)\n",
    "\n",
    "print(\"Profiles shape:\", profiles.shape)\n",
    "print(\"Listens shape:\", listens.shape)\n",
    "display(profiles.head())\n",
    "display(listens.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0eee11",
   "metadata": {},
   "source": [
    "# 2. Train/Test split by time\n",
    "\n",
    "We divide the dataset into a training set and a test set, using the last 30 days of listening events as the test period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f168ec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train listens: (1943981, 6)  Test listens: (42215, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_name</th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 23:08:57+00:00</td>\n",
       "      <td>f1b1cf71-bd35-4e99-8624-24a6e15f133a</td>\n",
       "      <td>Deep Dish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck Me Im Famous (Pacha Ibiza)-09-28-2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:54:10+00:00</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Composition 0919 (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:52:04+00:00</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mc2 (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:42:52+00:00</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hibari (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:42:11+00:00</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mc1 (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id                 timestamp  \\\n",
       "0  user_000001 2009-05-04 23:08:57+00:00   \n",
       "1  user_000001 2009-05-04 13:54:10+00:00   \n",
       "2  user_000001 2009-05-04 13:52:04+00:00   \n",
       "3  user_000001 2009-05-04 13:42:52+00:00   \n",
       "4  user_000001 2009-05-04 13:42:11+00:00   \n",
       "\n",
       "                                item_id  item_name track_id  \\\n",
       "0  f1b1cf71-bd35-4e99-8624-24a6e15f133a  Deep Dish      NaN   \n",
       "1  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8       坂本龍一      NaN   \n",
       "2  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8       坂本龍一      NaN   \n",
       "3  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8       坂本龍一      NaN   \n",
       "4  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8       坂本龍一      NaN   \n",
       "\n",
       "                                   track_name  \n",
       "0  Fuck Me Im Famous (Pacha Ibiza)-09-28-2007  \n",
       "1           Composition 0919 (Live_2009_4_15)  \n",
       "2                        Mc2 (Live_2009_4_15)  \n",
       "3                     Hibari (Live_2009_4_15)  \n",
       "4                        Mc1 (Live_2009_4_15)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_name</th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47454</th>\n",
       "      <td>user_000011</td>\n",
       "      <td>2009-06-17 17:00:58+00:00</td>\n",
       "      <td>2f3dfafb-be37-4d6c-86c7-23d0650497d4</td>\n",
       "      <td>Laineen Kasperi</td>\n",
       "      <td>31459b6a-c642-4786-b102-d42377f7922e</td>\n",
       "      <td>Järjen Häivii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47455</th>\n",
       "      <td>user_000011</td>\n",
       "      <td>2009-06-08 11:53:00+00:00</td>\n",
       "      <td>d2f84ebe-ce01-4612-9b1d-cd016e07cb88</td>\n",
       "      <td>Flash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Open Sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47456</th>\n",
       "      <td>user_000011</td>\n",
       "      <td>2009-06-06 15:30:41+00:00</td>\n",
       "      <td>bd13909f-1c29-4c27-a874-d4aaf27c5b1a</td>\n",
       "      <td>Fleetwood Mac</td>\n",
       "      <td>1988e204-96d5-49b0-bf73-450c572780d4</td>\n",
       "      <td>Black Magic Woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47457</th>\n",
       "      <td>user_000011</td>\n",
       "      <td>2009-06-05 22:03:46+00:00</td>\n",
       "      <td>bd13909f-1c29-4c27-a874-d4aaf27c5b1a</td>\n",
       "      <td>Fleetwood Mac</td>\n",
       "      <td>4cbbb2f9-b5d1-43ce-8aea-b281095a9564</td>\n",
       "      <td>Stop Messin' Round</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47458</th>\n",
       "      <td>user_000011</td>\n",
       "      <td>2009-06-05 22:00:54+00:00</td>\n",
       "      <td>bd13909f-1c29-4c27-a874-d4aaf27c5b1a</td>\n",
       "      <td>Fleetwood Mac</td>\n",
       "      <td>cb47245d-0489-4b59-b203-378a2221b2fb</td>\n",
       "      <td>Looking For Somebody</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_id                 timestamp  \\\n",
       "47454  user_000011 2009-06-17 17:00:58+00:00   \n",
       "47455  user_000011 2009-06-08 11:53:00+00:00   \n",
       "47456  user_000011 2009-06-06 15:30:41+00:00   \n",
       "47457  user_000011 2009-06-05 22:03:46+00:00   \n",
       "47458  user_000011 2009-06-05 22:00:54+00:00   \n",
       "\n",
       "                                    item_id        item_name  \\\n",
       "47454  2f3dfafb-be37-4d6c-86c7-23d0650497d4  Laineen Kasperi   \n",
       "47455  d2f84ebe-ce01-4612-9b1d-cd016e07cb88            Flash   \n",
       "47456  bd13909f-1c29-4c27-a874-d4aaf27c5b1a    Fleetwood Mac   \n",
       "47457  bd13909f-1c29-4c27-a874-d4aaf27c5b1a    Fleetwood Mac   \n",
       "47458  bd13909f-1c29-4c27-a874-d4aaf27c5b1a    Fleetwood Mac   \n",
       "\n",
       "                                   track_id            track_name  \n",
       "47454  31459b6a-c642-4786-b102-d42377f7922e         Järjen Häivii  \n",
       "47455                                   NaN              Open Sky  \n",
       "47456  1988e204-96d5-49b0-bf73-450c572780d4     Black Magic Woman  \n",
       "47457  4cbbb2f9-b5d1-43ce-8aea-b281095a9564    Stop Messin' Round  \n",
       "47458  cb47245d-0489-4b59-b203-378a2221b2fb  Looking For Somebody  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Train/Test split by time ===\n",
    "\n",
    "def split_train_test_time(df: pd.DataFrame, test_days: int = 7):\n",
    "    \"\"\"\n",
    "    Split interactions into train/test by time (global cutoff).\n",
    "    Train = all interactions before cutoff.\n",
    "    Test  = all interactions after cutoff.\n",
    "    \"\"\"\n",
    "    cutoff = df[\"timestamp\"].max() - pd.Timedelta(days=test_days)\n",
    "    train_df = df[df[\"timestamp\"] <= cutoff].copy()\n",
    "    test_df  = df[df[\"timestamp\"]  > cutoff].copy()\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = split_train_test_time(listens, test_days=30)\n",
    "\n",
    "print(\"Train listens:\", train_df.shape, \" Test listens:\", test_df.shape)\n",
    "display(train_df.head(), test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4fc851",
   "metadata": {},
   "source": [
    "# 3. Remove Re-Watched Items from Test Set\n",
    "\n",
    "Following Hu et al. (2008), we remove from the test set all (user, item) pairs that already appeared in the training set.  \n",
    "\n",
    "This ensures that the evaluation focuses on **novelty** (first-time recommendations) instead of rewarding trivial re-listening, which would artificially inflate the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8155ed46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test listens (filtered): (13512, 6)\n"
     ]
    }
   ],
   "source": [
    "# === Remove “re-watches” from test (optional, as in paper) ===\n",
    "def remove_rewatched_from_test(train_df: pd.DataFrame, test_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove from test the (user,item) pairs that appear in train,\n",
    "    to focus evaluation on discovery (first-time recommendations).\n",
    "    \"\"\"\n",
    "\n",
    "    # Keep only the unique (user,item) pairs from the training set\n",
    "    train_pairs = train_df[[\"user_id\", \"item_id\"]].drop_duplicates()\n",
    "\n",
    "    # Merge test with train pairs to flag which interactions already existed in train\n",
    "    merged = test_df.merge(train_pairs, on=[\"user_id\", \"item_id\"], how=\"left\", indicator=True)\n",
    "\n",
    "    # Keep only the test interactions that do not appear in train (\"left_only\")\n",
    "    filtered = merged[merged[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n",
    "\n",
    "    return filtered.reset_index(drop=True)  # Return the filtered test set, reindexed for cleanliness\n",
    "\n",
    "test_df_filtered = remove_rewatched_from_test(train_df, test_df)\n",
    "print(\"Test listens (filtered):\", test_df_filtered.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5766c02",
   "metadata": {},
   "source": [
    "Test set reduced to 13.512 interactions (from 42.214); still sufficient for meaningful Recall@K evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa1dae",
   "metadata": {},
   "source": [
    "# 4. Build User–Item Utility matrix\n",
    "\n",
    "Map user/item IDs to indices and build sparse CSR matrices (`train_matrix`, `test_matrix`) with per-(user, item) listen counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2e0189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSR shapes -> train: (499, 46736), test: (499, 46736), nnz(train)=204425, density=0.8766%\n"
     ]
    }
   ],
   "source": [
    "# === Build ID maps from TRAIN and CSR Utilty matrices ===\n",
    "\n",
    "# Transform user/item ids to contiguous indices\n",
    "user_ids = profiles[\"user_id\"].tolist()\n",
    "item_ids = sorted(train_df[\"item_id\"].unique().tolist())  # items seen in TRAIN only\n",
    "\n",
    "# Create dictionaries for id\n",
    "user2idx = {u: i for i, u in enumerate(user_ids)} # user-id to index mapping\n",
    "item2idx = {it: i for i, it in enumerate(item_ids)} # item-id to index mapping\n",
    "\n",
    "# Inverse mappings\n",
    "idx2user = {i: u for u, i in user2idx.items()}\n",
    "idx2item = {i: it for it, i in item2idx.items()}\n",
    "\n",
    "def df_to_csr_counts(df: pd.DataFrame,\n",
    "                     user2idx: dict,\n",
    "                     item2idx: dict,\n",
    "                     shape: tuple | None = None) -> sparse.csr_matrix:\n",
    "    \"\"\"\n",
    "    Build a CSR user-item matrix with counts of interactions.\n",
    "    Rows = users\n",
    "    Cols = items\n",
    "    Data = counts.\n",
    "    \"\"\"\n",
    "\n",
    "    # count how many times each (user, item) pair appears\n",
    "    grouped = df.groupby([\"user_id\", \"item_id\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "    # map user_id and item_id to integer indices\n",
    "    grouped[\"row\"] = grouped[\"user_id\"].map(user2idx)\n",
    "    grouped[\"col\"] = grouped[\"item_id\"].map(item2idx) #\n",
    "\n",
    "    # drop rows where either user or item is not in the dictionaries (cold-start cases)\n",
    "    grouped = grouped.dropna(subset=[\"row\", \"col\"]) \n",
    "\n",
    "    # extract rows, cols, and counts as NumPy arrays\n",
    "    rows = grouped[\"row\"].astype(int).to_numpy()\n",
    "    cols = grouped[\"col\"].astype(int).to_numpy() \n",
    "    data = grouped[\"count\"].astype(np.float32).to_numpy() \n",
    "\n",
    "    # build the sparse CSR matrix\n",
    "    n_users, n_items = len(user2idx), len(item2idx)\n",
    "    if not shape:\n",
    "        shape = (n_users, n_items)\n",
    "    return sparse.csr_matrix((data, (rows, cols)), shape=shape, dtype=np.float32)\n",
    "\n",
    "# Build utility matrices (users x items, counts)\n",
    "train_matrix = df_to_csr_counts(train_df, user2idx, item2idx)\n",
    "test_matrix  = df_to_csr_counts(test_df_filtered, user2idx, item2idx, shape=train_matrix.shape) \n",
    "\n",
    "n_users, n_items = train_matrix.shape\n",
    "density = train_matrix.nnz / (n_users * n_items) * 100\n",
    "\n",
    "print(f\"CSR shapes -> train: {train_matrix.shape}, test: {test_matrix.shape}, nnz(train)={train_matrix.nnz}, density={density:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e534ed2",
   "metadata": {},
   "source": [
    "**Utility Matrix Sparsity**\n",
    "\n",
    "The training matrix has shape **(499 × 46.736)**, meaning ~23.3 million possible user–item pairs.  \n",
    "Out of these, number of non-zero entries are **204.425**, i.e. real user–item interactions.  \n",
    "\n",
    "This yields a density of ~**0.9%**, confirming that the utility matrix is **highly sparse**. \n",
    "Such sparsity is typical of recommender system datasets and motivates the use of **sparse matrix representations** and **collaborative filtering methods** designed for sparse data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b35fe4",
   "metadata": {},
   "source": [
    "## 5. Baseline: Global Popularity\n",
    "\n",
    "As a simple non-personalized baseline, we recommend the globally most popular items:  \n",
    "- Compute total counts per item in the training set.  \n",
    "- For each user, mask already-seen items.  \n",
    "- Recommend the top-K unseen items.  \n",
    "\n",
    "This provides a lower-bound reference for evaluation: any personalized method should \n",
    "outperform the popularity baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7a32b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Popularity] Recall@10: 0.0063\n"
     ]
    }
   ],
   "source": [
    "# === Baseline: Global Popularity ===\n",
    "\n",
    "def get_seen_items(csr_matrix: sparse.csr_matrix, u: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return the indices of items with non-zero interactions for user u.\n",
    "    Uses CSR row pointers (indptr) to quickly extract the indices.\n",
    "    \"\"\"\n",
    "    start, end = csr_matrix.indptr[u], csr_matrix.indptr[u+1]\n",
    "    return csr_matrix.indices[start:end]\n",
    "\n",
    "def baseline_popularity_recall_at_k(train_csr: sparse.csr_matrix,\n",
    "                                    test_csr: sparse.csr_matrix,\n",
    "                                    K: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Compute Recall@K for a popularity-based recommender.\n",
    "    For each user u:\n",
    "    - Recommend the K most popular items in train (excluding already-seen items).\n",
    "    - Compare with test items to compute hits.\n",
    "    \"\"\"\n",
    "    item_pop = np.asarray(train_csr.sum(axis=0)).flatten() # total counts per item\n",
    "    popular_order = np.argsort(-item_pop)\n",
    "\n",
    "    hits, total = 0, 0 # hit count and total relevant items \n",
    "\n",
    "    for u in range(train_csr.shape[0]):\n",
    "\n",
    "        test_items = get_seen_items(test_csr, u) # items in test for user u\n",
    "        if test_items.size == 0:\n",
    "            continue # skip users with no test items\n",
    "        \n",
    "        # Extract items seen in train for user u\n",
    "        seen = set(get_seen_items(train_csr, u).tolist()) # list of items consumed by user u in train\n",
    "        reccomendations = [i for i in popular_order if i not in seen][:K] # top-K popular not seen\n",
    "        hits += len(set(reccomendations).intersection(set(test_items.tolist())))\n",
    "        total += len(test_items)\n",
    "    return hits / total if total > 0 else np.nan\n",
    "\n",
    "K = 10\n",
    "pop_rec = baseline_popularity_recall_at_k(train_matrix, test_matrix, K=K)\n",
    "print(f\"[Popularity] Recall@{K}: {pop_rec:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf8676",
   "metadata": {},
   "source": [
    "Recall@10 values are low due to dataset sparsity, but relative improvements (Popularity < Item–Item < ALS) remain consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd1382",
   "metadata": {},
   "source": [
    "**Evaluation Metrics**\n",
    "\n",
    "We evaluate recommendations using **ranking-based metrics** tailored for implicit feedback:\n",
    "\n",
    "- **Recall@K**  \n",
    "  Proportion of relevant test items appearing in the top-K recommendations.  \n",
    "  *Higher is better* (0 = none recovered, 1 = all recovered).\n",
    "\n",
    "- **MAP@K (Mean Average Precision)**  \n",
    "  Accounts for both the presence and the ranking of relevant items.  \n",
    "  Rewards models that rank relevant items earlier in the list.  \n",
    "  *Higher is better.*\n",
    "\n",
    "- **Expected Percentile Ranking (EPR)**  \n",
    "  Average percentile rank of relevant test items across the full recommendation list.  \n",
    "  *Lower is better* (≈50% = random ranking; closer to 0% = ideal ranking at the top).\n",
    "\n",
    "**Summary:**  \n",
    "- Recall@K → higher is better (“how many relevant items were recovered”)  \n",
    "- MAP@K → higher is better (“how well they were ordered”)  \n",
    "- EPR → lower is better (“how high they appear on average”)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db8768",
   "metadata": {},
   "source": [
    "## 6. Item–Item Collaborative Filtering (Cosine)\n",
    "\n",
    "We implement a memory-based baseline using **cosine similarity** between item vectors:  \n",
    "- Each item is represented as a vector of user interactions.  \n",
    "- Cosine similarity measures co-consumption by the same users.  \n",
    "- For each user, we score unseen items as the weighted sum of similarities with items already consumed.  \n",
    "\n",
    "This method personalizes recommendations by leveraging similarity between items, but does not exploit confidence weighting, making it a simpler baseline compared to ALS.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3ba4b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Item-Item Cosine] Recall@10: 0.0094\n"
     ]
    }
   ],
   "source": [
    "# === Baseline: Item-Item Collaborative Filtering (Cosine) ===\n",
    "\n",
    "def item_item_cosine_scores(train_csr: sparse.csr_matrix) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute item-item cosine similarity matrix S (I x I).\n",
    "    \"\"\"\n",
    "    X = train_csr.T.tocsr().astype(np.float64)  # Transpose to items x users.\n",
    "    \n",
    "    #L2-normalize item rows\n",
    "    norms = np.sqrt(X.multiply(X).sum(axis=1)).A1 + 1e-12 # avoid div by zero\n",
    "    Xn = X.multiply(1.0 / norms[:, None]) # normalize rows\n",
    "    S = Xn @ Xn.T # cosine similarity matrix\n",
    "    return S\n",
    "\n",
    "S = item_item_cosine_scores(train_matrix)\n",
    "\n",
    "def cosine_recall_at_k(S: sparse.spmatrix | np.ndarray, train_csr, test_csr, K=10) -> float:\n",
    "    \"\"\"\n",
    "    For each user u:\n",
    "    - Build a binary item profile over seen train items.\n",
    "    - Score candidates as the sum of cosine similarities to seen items.\n",
    "    - Mask seen items, extract top-K, and count hits against test items.\n",
    "    \"\"\"\n",
    "\n",
    "    hits, total = 0, 0\n",
    "    for u in range(train_csr.shape[0]):\n",
    "        seen = get_seen_items(train_csr, u)\n",
    "        user_profile = np.zeros(S.shape[0], dtype=np.float64)\n",
    "        user_profile[seen] = 1.0\n",
    "        scores = user_profile @ S # score vector for affinity of user u to all items\n",
    "        scores[seen] = -np.inf\n",
    "\n",
    "        recs = np.argpartition(-scores, K)[:K]\n",
    "        recs = recs[np.argsort(-scores[recs])]\n",
    "        test_items = get_seen_items(test_csr, u)\n",
    "        if test_items.size == 0:\n",
    "            continue\n",
    "        hits += len(set(recs).intersection(set(test_items.tolist())))\n",
    "        total += len(test_items)\n",
    "    return hits / total if total > 0 else np.nan\n",
    "\n",
    "cos_rec = cosine_recall_at_k(S, train_matrix, test_matrix, K=10)\n",
    "print(f\"[Item-Item Cosine] Recall@{K}: {cos_rec:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5617e",
   "metadata": {},
   "source": [
    "## 7. Implicit ALS (Alternating Least Squares)\n",
    "\n",
    "We implement the **ALS algorithm for implicit feedback** following Hu, Koren & Volinsky (2008).  \n",
    "\n",
    "Key ideas:  \n",
    "- Each user and item is represented by a latent factor vector.  \n",
    "- Binary preferences \\(p_{ui}\\) are derived from implicit feedback (1 if user interacted, 0 otherwise).  \n",
    "- Confidence weights \\( c_{ui} = 1 + alpha * f(r_{ui} )\\) scale the importance of frequent interactions.  \n",
    "- The model alternates between updating user factors (X) and item factors (Y)  \n",
    "  by solving regularized least-squares systems.  \n",
    "- Variants:  \n",
    "  - **Linear confidence**: \\(f(r) = r\\)  \n",
    "  - **Log confidence**: \\(f(r) = log(1 + r/c_0)\\)  \n",
    "\n",
    "This factorization captures hidden patterns beyond direct co-consumption, making ALS more powerful than memory-based methods such as Item–Item CF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f33e7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Implicit ALS implementation ===\n",
    "\n",
    "class ImplicitALS:\n",
    "    \"\"\"\n",
    "    Alternating Least Squares for Implicit Feedback\n",
    "    Reference:\n",
    "      Hu, Koren, Volinsky (2008), 'Collaborative Filtering for Implicit Feedback Datasets'\n",
    "      - min_x,y sum_{u,i} c_ui (p_ui - x_u^T y_i)^2 + lambda (||x||^2 + ||y||^2)\n",
    "      - p_ui = 1 if r_ui > 0 else 0\n",
    "      - c_ui = 1 + alpha * r_ui   (or log-based)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_items, n_factors=64, n_iters=10, \n",
    "                 reg=0.1, alpha=40.0, use_log_conf=False, log_eps=1e-8):\n",
    "        \"\"\"\n",
    "        n_users: number of users\n",
    "        n_items: number of items\n",
    "        n_factors: number of latent factors\n",
    "        n_iters: number of ALS iterations\n",
    "        reg: regularization parameter (lambda)\n",
    "        alpha: confidence scaling factor\n",
    "        use_log_conf: if True, use log-based confidence\n",
    "        log_eps: small constant for log-based confidence\n",
    "        \"\"\"\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_factors = n_factors\n",
    "        self.n_iters = n_iters\n",
    "        self.reg = reg\n",
    "        self.alpha = alpha\n",
    "        self.use_log_conf = use_log_conf\n",
    "        self.log_eps = log_eps\n",
    "        # Factors (initialized with small random values)\n",
    "        self.X = 0.01 * np.random.randn(n_users, n_factors) # user factors\n",
    "        self.Y = 0.01 * np.random.randn(n_items, n_factors) # item factors\n",
    "\n",
    "    def _conf_from_counts(self, counts: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute confidence from counts:\n",
    "        c_ui = 1 + alpha * f(r_ui).\n",
    "        \n",
    "        or for log-based:\n",
    "        c_ui = 1 + alpha * log(1 + r_ui / c0),\n",
    "        c0 is the mean of counts, used to normalize them.\n",
    "        \"\"\"\n",
    "        if self.use_log_conf:\n",
    "            \n",
    "            c0 = max(1.0, np.mean(counts))\n",
    "            return 1.0 + self.alpha * np.log(1.0 + counts / c0)\n",
    "        else:\n",
    "            return 1.0 + self.alpha * counts\n",
    "\n",
    "    def fit(self, train_counts: sparse.csr_matrix):\n",
    "        \"\"\"\n",
    "        Train ALS alternating users/items updates.\n",
    "        train_counts: CSR matrix (#users x #items).\n",
    "        \"\"\"\n",
    "        train_counts_csr_items = train_counts.tocsr() # train_counts in CSR (compressed sparse row)\n",
    "        train_counts_csc_users = train_counts.tocsc() # train_counts in CSC (compressed sparse column)\n",
    "\n",
    "        Id_mat = np.eye(self.n_factors, dtype=np.float64) # Identity matrix (f,f)\n",
    "\n",
    "        # Main ALS loop\n",
    "        for it in range(self.n_iters):\n",
    "\n",
    "            # Precompute Y^T Y and X^T X once per half-iteration\n",
    "            YtY = self.Y.T @ self.Y  # matricial product\n",
    "\n",
    "            # === Update user factors ===\n",
    "            for u in range(self.n_users):\n",
    "                row_start, row_end = train_counts_csr_items.indptr[u], train_counts_csr_items.indptr[u+1]\n",
    "                item_idx = get_seen_items(train_counts_csr_items, u) # list of items consumed by user u\n",
    "                \n",
    "                if item_idx.size == 0:\n",
    "                    # cold user\n",
    "                    A = YtY + self.reg * Id_mat\n",
    "                    b = np.zeros(self.n_factors, dtype=np.float64)\n",
    "                    self.X[u, :] = np.linalg.solve(A, b)\n",
    "                    continue\n",
    "                \n",
    "                r_u = train_counts_csr_items.data[row_start:row_end]  # lists of counts for items consumed by user u\n",
    "                c_u = self._conf_from_counts(r_u)               # confidence values\n",
    "                c_u_minus1 = c_u - 1.0\n",
    "                Y_u = self.Y[item_idx, :]                       # factors of items consumed by user u\n",
    "                \n",
    "                # A = Y^T Y + Y_u^T * diag(c_u - 1) * Y_u + reg*I\n",
    "                A = YtY + (Y_u.T * c_u_minus1) @ Y_u + self.reg * Id_mat\n",
    "                \n",
    "                # b = Y_u^T * (c_u * p_u) ;  p_u = 1 for items seen by user u\n",
    "                weighted_u = Y_u * c_u.reshape(-1,1)\n",
    "                b = weighted_u.sum(axis=0)\n",
    "\n",
    "                # update user factors\n",
    "                self.X[u, :] = np.linalg.solve(A, b) # solve for x_u\n",
    "\n",
    "            # === Update item factors ===\n",
    "\n",
    "            XtX = self.X.T @ self.X # matricial product\n",
    "\n",
    "            for i in range(self.n_items):\n",
    "                col_start, col_end = train_counts_csc_users.indptr[i], train_counts_csc_users.indptr[i+1]\n",
    "                user_idx = train_counts_csc_users.indices[col_start:col_end] # list of users that consumed item i\n",
    "                \n",
    "                if user_idx.size == 0:\n",
    "                    # cold item\n",
    "                    A = XtX + self.reg * Id_mat\n",
    "                    b = np.zeros(self.n_factors, dtype=np.float64)\n",
    "                    self.Y[i, :] = np.linalg.solve(A, b)\n",
    "                    continue\n",
    "                r_i = train_counts_csc_users.data[col_start:col_end]  # lists of counts for users that consumed item i\n",
    "                c_i = self._conf_from_counts(r_i)        # confidence values\n",
    "                c_i_minus1 = c_i - 1.0\n",
    "                X_i = self.X[user_idx, :]                # factors of users that consumed item i\n",
    "\n",
    "                # A = X^T X + X_i^T * diag(c_i - 1) * X_i + reg*I                      \n",
    "                A = XtX + (X_i.T * c_i_minus1) @ X_i + self.reg * Id_mat\n",
    "\n",
    "                # b = X_i^T * (c_i * p_i) ;  p_i = 1 for users that consumed item i\n",
    "                weighted_i = X_i * c_i.reshape(-1,1)\n",
    "                b = weighted_i.sum(axis=0)\n",
    "\n",
    "                # update item factors\n",
    "                self.Y[i, :] = np.linalg.solve(A, b)\n",
    "\n",
    "\n",
    "    def recommend_for_user(self, u: int, K: int = 10, \n",
    "                           seen_items: np.ndarray | None = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return top-K item indices for user u, \n",
    "        masking 'seen_items' if provided.\n",
    "        \"\"\"\n",
    "        scores = self.X[u, :] @ self.Y.T # score vector for affinity of user u to all items\n",
    "\n",
    "        if seen_items is not None and seen_items.size > 0:\n",
    "            scores[seen_items] = -np.inf # mask seen items\n",
    "\n",
    "        topk = np.argsort(-scores)[:K] # get top-K indices\n",
    "        return topk # return top-K item indices\n",
    "    \n",
    "\n",
    "    def full_scores(self) -> np.ndarray:\n",
    "        \"\"\"Compute dense score matrix R: X @ Y^T .\"\"\"\n",
    "        return self.X @ self.Y.T\n",
    "    \n",
    "\n",
    "    def explain(self, u: int, i: int, train_counts: sparse.csr_matrix, top_m: int = 5):\n",
    "        \"\"\"\n",
    "        Explain why item i is recommended to user u.\n",
    "\n",
    "        For a target user u and a candidate item i, compute how much each item j\n",
    "        previously consumed by u contributes to the predicted score p_hat_ui.\n",
    "        \n",
    "        For each item j consumed by u, compute contribution s_ij^u * c_uj,\n",
    "        where s_ij^u = y_i^T W_u y_j, \n",
    "        with W_u = A^(-1),\n",
    "        and A = Y^T C_u Y + reg I.\n",
    "        \n",
    "        from: p_hat_ui = sum_{j: r_uj>0} s_ij^u * c_uj.\n",
    "        \"\"\"\n",
    "        # Extract items consumed by user u from CSR\n",
    "        row_start, row_end = train_counts.indptr[u], train_counts.indptr[u+1]\n",
    "        item_idx = get_seen_items(train_counts, u) # list of items consumed by user u\n",
    "        if item_idx.size == 0:\n",
    "            return []\n",
    "\n",
    "        # Get raw counts and corresponding confidences\n",
    "        r_u = train_counts.data[row_start:row_end]\n",
    "        c_u = self._conf_from_counts(r_u)      \n",
    "        c_u_minus1 = c_u - 1.0\n",
    "\n",
    "        # Build A and its inverse W_u\n",
    "        Y_u = self.Y[item_idx, :]\n",
    "        YtY = self.Y.T @ self.Y\n",
    "        I_f = np.eye(self.n_factors, dtype=np.float64)\n",
    "        A = YtY + (Y_u.T * c_u_minus1) @ Y_u + self.reg * I_f\n",
    "\n",
    "        W_u = np.linalg.inv(A) # inverse of A\n",
    "\n",
    "        # Compute contributions s_ij^u for each item j consumed by user u\n",
    "\n",
    "        y_i = self.Y[i, :][:, None]  # factor of item i\n",
    "\n",
    "        # s_ij^u = y_i^T W_u y_j  => scalar for each j consumed\n",
    "        s_ij = (y_i.T @ W_u @ Y_u.T).flatten()  # how much each item j contributes to item i\n",
    "        contrib = s_ij * c_u                   # weight by confidence\n",
    "\n",
    "        # Get top-m contributions\n",
    "        order = np.argsort(-contrib)[:top_m] # indices of top-m contributions\n",
    "        return [(int(item_idx[j]), float(contrib[j]), float(c_u[j])) for j in order] # return top-m (item, contribution, confidence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c8e45",
   "metadata": {},
   "source": [
    "we use now the parameters following the paper: alpha = 40, fact = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a65f31fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Train ALS (linear confidence) ===\n",
    "\n",
    "np.random.seed(RNG_SEED)\n",
    "als_linear = ImplicitALS(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    n_factors=64, # latent factors\n",
    "    n_iters=10,   # number of ALS iterations\n",
    "    reg=0.1,      # regularization factor\n",
    "    alpha=40.0,   # confidence scaling\n",
    "    use_log_conf=False, # log-based confidence\n",
    ")\n",
    "als_linear.fit(train_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246564d",
   "metadata": {},
   "source": [
    "**Metrics evaluation**\n",
    "\n",
    "We define functions to compute ranking-based metrics also for the ALS model:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e501aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(model: ImplicitALS, train_csr: sparse.csr_matrix, test_csr: sparse.csr_matrix, K: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Compute micro-averaged Recall@K for an implicit ALS model.\n",
    "\n",
    "    For each user u:\n",
    "      1) Collect test items (ground-truth positives).\n",
    "      2) Generate top-K recommendations from the model, masking items seen in train.\n",
    "      3) Count hits and aggregate over all users with non-empty test.\n",
    "    \"\"\" \n",
    "    hits, total = 0, 0 # hit count and total relevant items\n",
    "    for u in range(model.n_users):\n",
    "\n",
    "        test_items = get_seen_items(test_csr, u)\n",
    "        if test_items.size == 0:\n",
    "            continue # skip users with no test items\n",
    "\n",
    "        seen = get_seen_items(train_csr, u) # items seen in train for user u\n",
    "        recs = model.recommend_for_user(u, K=K, seen_items=seen) # recommended k items not seen\n",
    "        hits += len(set(recs).intersection(set(test_items)))\n",
    "        total += len(test_items)\n",
    "    return hits / total if total > 0 else np.nan\n",
    "\n",
    "def average_precision_k(actual: list[int], predicted: list[int], k: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Compute Average Precision at K (AP@K) for a single user.\n",
    "    actual: list of ground truth relevant item indices\n",
    "    predicted: list of predicted item indices (ordered by relevance)\n",
    "    k: consider only the top-k predictions\n",
    "    Returns AP@K score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize types\n",
    "    if isinstance(actual, np.ndarray):\n",
    "        actual_set = set(int(a) for a in actual.tolist())\n",
    "    else:\n",
    "        actual_set = set(int(a) for a in actual)\n",
    "    if len(actual_set) == 0:  # <-- niente \"if actual\"\n",
    "        return 0.0\n",
    "\n",
    "    # Convert predicted to list of ints and truncate to k\n",
    "    pred_list = list(int(p) for p in (predicted.tolist() if isinstance(predicted, np.ndarray) else predicted))\n",
    "    if len(pred_list) > k:\n",
    "        pred_list = pred_list[:k]\n",
    "\n",
    "    score, hits = 0.0, 0.0\n",
    "    seen = set()\n",
    "    for i, p in enumerate(pred_list, start=1):\n",
    "        if p in actual_set and p not in seen:\n",
    "            hits += 1.0\n",
    "            score += hits / i  # precision at i\n",
    "            seen.add(p)\n",
    "    return score / min(len(actual_set), k)\n",
    "\n",
    "def map_at_k(model: ImplicitALS, train_csr: sparse.csr_matrix, test_csr: sparse.csr_matrix, K: int = 10) -> float:\n",
    "    \"\"\"\"\n",
    "    Mean Average Precision at K (MAP@K).\n",
    "    For each user, compute AP@K and average over all users.\n",
    "    \"\"\"\n",
    "    scores, n = 0.0, 0\n",
    "    for u in range(model.n_users):\n",
    "        \n",
    "        test_items = get_seen_items(test_csr, u) # items in test for user u\n",
    "        if test_items.size == 0:\n",
    "            continue\n",
    "        seen = get_seen_items(train_csr, u)\n",
    "        recs = model.recommend_for_user(u, K=K, seen_items=seen).tolist()\n",
    "        scores += average_precision_k(test_items, recs, k=K)\n",
    "        n += 1\n",
    "    return scores / n if n > 0 else np.nan\n",
    "\n",
    "def expected_percentile_ranking(model: ImplicitALS, train_csr: sparse.csr_matrix, test_csr: sparse.csr_matrix) -> float:\n",
    "    \"\"\"\n",
    "    Compute Expected Percentile Ranking (EPR).\n",
    "    For each user u and each test item i ∈ Test_u, compute the percentile rank\n",
    "    of i within the full ranking of all items scored for u (0% = best, 100% = worst),\n",
    "    after masking training-seen items to avoid trivial ranks. \n",
    "    Return the mean over all (u, i) pairs. \n",
    "    \"\"\"\n",
    "    scores = model.full_scores()  # (U, I)\n",
    "    # Mask seen items to avoid trivial ranks\n",
    "    for u in range(model.n_users):\n",
    "        seen = get_seen_items(train_csr, u)\n",
    "        if seen.size:\n",
    "            scores[u, seen] = -np.inf \n",
    "\n",
    "    # For each positive (u,i) in test, compute percentile rank\n",
    "    ranks = []\n",
    "    n_items = scores.shape[1]\n",
    "    for u in range(model.n_users):\n",
    "        s = scores[u, :]\n",
    "        if not np.isfinite(s).any(): # all -inf (cold user)\n",
    "            continue\n",
    "\n",
    "        order = np.argsort(-s)  # descending\n",
    "        inv_rank = np.empty_like(order)\n",
    "        inv_rank[order] = np.arange(n_items)  # 0 = best\n",
    "        items = get_seen_items(test_csr, u)\n",
    "\n",
    "        for i in items:\n",
    "            r = inv_rank[i] / (n_items - 1) * 100.0  # percentile\n",
    "            ranks.append(r)\n",
    "    return float(np.mean(ranks)) if ranks else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d025ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ALS (linear)] Recall@10: 0.0063\n",
      "[ALS (linear)] MAP@10:    0.0052\n",
      "[ALS (linear)] EPR:        22.73%\n"
     ]
    }
   ],
   "source": [
    "print(f\"[ALS (linear)] Recall@{K}: {recall_at_k(als_linear, train_matrix, test_matrix, K):.4f}\")\n",
    "print(f\"[ALS (linear)] MAP@{K}:    {map_at_k(als_linear, train_matrix, test_matrix, K):.4f}\")\n",
    "print(f\"[ALS (linear)] EPR:        {expected_percentile_ranking(als_linear, train_matrix, test_matrix):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5183ef",
   "metadata": {},
   "source": [
    "with this parameters ALS (linear) does not outperforms popularity and item–item CF in Recall@10. We'll see then with parametry sensitivity if improve.\n",
    "\n",
    "However, EPR is well below 50% (random), showing good personalization and ranking quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53951748",
   "metadata": {},
   "source": [
    "## 8. Example: Human-readable Recommendations and Explanations\n",
    "\n",
    "We inspect recommendations for a sample user:\n",
    "\n",
    "- **Top-K recommendations** are shown with artist names \n",
    "- The `explain` function analyzes the **top recommendation**, showing how previously consumed items contribute to its score.  \n",
    "\n",
    "This provides interpretability: we can see not only *what* is recommended, but also *why* the model ranked it highly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef1971a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User idx 242 (user_000243) — Top-5 recommendations:\n",
      "  45567  Bob Sinclar\n",
      "  10048  808 State\n",
      "  10930  Four Tet\n",
      "  28221  Bonobo\n",
      "  37343  Adele\n",
      "\n",
      "Explanations for top recommendation (item idx 45567 - Bob Sinclar):\n",
      "\n",
      "  Item idx 8840 (David Guetta): contribution=0.2968, confidence=881.0000\n",
      "  Item idx 24939 (Paranormal Attack): contribution=0.1706, confidence=1161.0000\n",
      "  Item idx 22244 (Deepest Blue): contribution=0.1456, confidence=1121.0000\n",
      "  Item idx 44621 (Jamiroquai): contribution=0.1257, confidence=601.0000\n",
      "  Item idx 30888 (Simply Red): contribution=0.1090, confidence=881.0000\n"
     ]
    }
   ],
   "source": [
    "# === Explain top recommendation for one user (human-readable) ===\n",
    "\n",
    "def topk_recs_with_names(model: ImplicitALS, u: int, K: int, train_csr, idx2item, item_name_lookup: dict):\n",
    "    \"\"\"Return list of (item_index, item_name) for top-K recommendations.\"\"\"\n",
    "    seen = get_seen_items(train_csr, u)\n",
    "    recs = model.recommend_for_user(u, K=K, seen_items=seen)\n",
    "    names = []\n",
    "    for i in recs:\n",
    "        item_id = idx2item[i]\n",
    "        names.append(item_name_lookup.get(item_id, str(item_id)))\n",
    "    return list(zip(recs, names))\n",
    "\n",
    "# Build a stable item_id -> item_name lookup from listens (most frequent name per item)\n",
    "name_map = (\n",
    "    listens.groupby([\"item_id\", \"item_name\"])\n",
    "           .size()\n",
    "           .reset_index(name=\"cnt\")\n",
    "           .sort_values([\"item_id\", \"cnt\"], ascending=[True, False])\n",
    "           .drop_duplicates(subset=[\"item_id\"])\n",
    "           .set_index(\"item_id\")[\"item_name\"]\n",
    "           .to_dict()\n",
    ")\n",
    "\n",
    "u_demo = 242     # change this index to inspect a different user\n",
    "K_demo = 5\n",
    "\n",
    "recs = topk_recs_with_names(als_linear, u_demo, K_demo, train_matrix, idx2item, name_map)\n",
    "print(f\"User idx {u_demo} ({idx2user[u_demo]}) — Top-{K_demo} recommendations:\")\n",
    "for i, nm in recs:\n",
    "    print(f\"  {i:5d}  {nm}\")\n",
    "\n",
    "# Explain top recommendation\n",
    "explanations = als_linear.explain(u_demo, recs[0][0], train_matrix, top_m=5)\n",
    "print(f\"\\nExplanations for top recommendation (item idx {recs[0][0]} - {recs[0][1]}):\\n\")\n",
    "\n",
    "for item_j, contrib, conf in explanations:\n",
    "    item_name = name_map.get(idx2item[item_j], str(idx2item[item_j]))\n",
    "    print(f\"  Item idx {item_j} ({item_name}): contribution={contrib:.4f}, confidence={conf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594eacf6",
   "metadata": {},
   "source": [
    "The model recommends artists like *Bob Sinclair*.  \n",
    "The explanation indicates that this choice is influenced by past listens (e.g., *David Guetta*, *Paranormal Attack*, ..),  \n",
    "with contributions weighted by both **confidence** (listening frequency) and **latent similarity** between items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da097afa",
   "metadata": {},
   "source": [
    "## 9. ALS vs ALS-LOG (comparison)\n",
    "\n",
    "We now compare the **linear confidence** version of ALS (as in the paper) with the **log-scaled variant** (optional extension).  \n",
    "\n",
    "- **ALS (linear)**: \\(c_{ui} = 1 + \\alpha r_{ui}\\)  \n",
    "- **ALS-LOG**: \\(c_{ui} = 1 + \\alpha \\log(1 + r_{ui}/c_0)\\)  \n",
    "\n",
    "The log-scaled version reduces the dominance of very frequent interactions and provides a robustness check for the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8fb1c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>MAP@10</th>\n",
       "      <th>EPR (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALS (linear)</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>22.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALS (log)</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>21.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model Recall@10  MAP@10 EPR (%)\n",
       "0  ALS (linear)    0.0063  0.0052   22.73\n",
       "1     ALS (log)    0.0088  0.0103   21.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train ALS (log confidence)\n",
    "\n",
    "np.random.seed(RNG_SEED)\n",
    "als_log = ImplicitALS(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    n_factors=64,\n",
    "    n_iters=10,\n",
    "    reg=0.1,\n",
    "    alpha=40.0,\n",
    "    use_log_conf=True,\n",
    ")\n",
    "als_log.fit(train_matrix)\n",
    "\n",
    "# Evaluate both\n",
    "def evaluate_model(name, model, train_mat, test_mat, K=10):\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        f\"Recall@{K}\": f\"{recall_at_k(model, train_mat, test_mat, K):.4f}\",\n",
    "        f\"MAP@{K}\":    f\"{map_at_k(model,    train_mat, test_mat, K):.4f}\",\n",
    "        \"EPR (%)\":     f\"{expected_percentile_ranking(model, train_mat, test_mat):.2f}\"\n",
    "    }\n",
    "\n",
    "K = 10\n",
    "results = [\n",
    "    evaluate_model(\"ALS (linear)\", als_linear, train_matrix, test_matrix, K),\n",
    "    evaluate_model(\"ALS (log)\",    als_log,    train_matrix, test_matrix, K)\n",
    "]\n",
    "\n",
    "als_compare_df = pd.DataFrame(results)\n",
    "display(als_compare_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e327aa",
   "metadata": {},
   "source": [
    " Compared to the linear version, ALS with log-scaled confidence achieves slightly more stable results, reducing the dominance of very frequent interactions while maintaining ranking quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63714551",
   "metadata": {},
   "source": [
    "## 10. Parameter Sensitivity\n",
    "\n",
    "We vary **α (confidence scaling)** and **latent factors (f)** to test ALS robustness.  \n",
    "\n",
    "- **α (alpha)**: scales confidence weights, controlling the impact of frequent interactions.  \n",
    "- **Number of latent factors (f)**: dimensionality of user/item embeddings.\n",
    "\n",
    "This follows the original paper, which highlights sensitivity mainly to α and f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2320c7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>alpha</th>\n",
       "      <th>factors</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>MAP@10</th>\n",
       "      <th>EPR (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALS (linear)</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>16.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALS (linear)</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>17.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALS (linear)</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>17.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALS (linear)</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>19.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALS (linear)</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>19.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ALS (linear)</td>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>21.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  alpha  factors Recall@10  MAP@10 EPR (%)\n",
       "0  ALS (linear)     10       16    0.0088  0.0072   16.20\n",
       "1  ALS (linear)     10       32    0.0151  0.0211   17.55\n",
       "2  ALS (linear)     40       16    0.0044  0.0050   17.73\n",
       "3  ALS (linear)     40       32    0.0107  0.0082   19.61\n",
       "4  ALS (linear)    100       16    0.0019  0.0008   19.77\n",
       "5  ALS (linear)    100       32    0.0038  0.0033   21.57"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>alpha</th>\n",
       "      <th>factors</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>MAP@10</th>\n",
       "      <th>EPR (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALS (log)</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALS (log)</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0218</td>\n",
       "      <td>16.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALS (log)</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>15.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALS (log)</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>17.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALS (log)</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>15.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ALS (log)</td>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>17.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  alpha  factors Recall@10  MAP@10 EPR (%)\n",
       "0  ALS (log)     10       16    0.0182  0.0214   15.33\n",
       "1  ALS (log)     10       32    0.0182  0.0218   16.75\n",
       "2  ALS (log)     40       16    0.0157  0.0219   15.47\n",
       "3  ALS (log)     40       32    0.0157  0.0161   17.05\n",
       "4  ALS (log)    100       16    0.0126  0.0146   15.45\n",
       "5  ALS (log)    100       32    0.0126  0.0153   17.47"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sweep_params(alphas=(10, 40, 100), factors=(16, 32), K=10, use_log_conf=False):\n",
    "    \"\"\"\n",
    "    Grid-search over (alpha, n_factors) for ImplicitALS and collect multiple metrics.\n",
    "    Reuses `evaluate_model` to compute Recall@K, MAP@K, and EPR in one pass.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for a in alphas:\n",
    "        for f in factors:\n",
    "            np.random.seed(RNG_SEED)\n",
    "            m = ImplicitALS(\n",
    "                n_users=n_users,\n",
    "                n_items=n_items,\n",
    "                n_factors=f,\n",
    "                n_iters=10,\n",
    "                reg=0.1,\n",
    "                alpha=a,\n",
    "                use_log_conf=use_log_conf,\n",
    "            )\n",
    "            m.fit(train_matrix)\n",
    "\n",
    "            # Reuse the existing helper to compute all metrics\n",
    "            row = evaluate_model(\n",
    "                name=f\"ALS ({'log' if use_log_conf else 'linear'})\",\n",
    "                model=m,\n",
    "                train_mat=train_matrix,\n",
    "                test_mat=test_matrix,\n",
    "                K=K\n",
    "            )\n",
    "            # Add the hyperparameters as explicit columns\n",
    "            row.update({\"alpha\": a, \"factors\": f})\n",
    "            rows.append(row)\n",
    "    \n",
    "    # Build a tidy DataFrame and order columns nicely\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Reorder columns: Model → alpha → factors → metrics\n",
    "    metric_cols = [f\"Recall@{K}\", f\"MAP@{K}\", \"EPR (%)\"]\n",
    "    ordered_cols = [\"Model\", \"alpha\", \"factors\"] + metric_cols\n",
    "    df = df[ordered_cols].sort_values([\"Model\", \"alpha\", \"factors\"]).reset_index(drop=True)       \n",
    "\n",
    "    return df\n",
    "\n",
    "sweep_linear = sweep_params(use_log_conf=False, K=10)\n",
    "display(sweep_linear)\n",
    "\n",
    "sweep_log = sweep_params(use_log_conf=True, K=10)\n",
    "display(sweep_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f16dcc3",
   "metadata": {},
   "source": [
    "Log-scaled confidence proves more robust than linear, with higher Recall/MAP and lower EPR.  \n",
    "The best setting for ALS is `alpha=10, n_factors=16`. \n",
    "\n",
    "Unlike the original paper (which favors larger α and dimensions on big datasets), smaller values perform better here due to the reduced sample size.  \n",
    "  \n",
    "- **Lower α** works better, since high values would overweight the few repeated listens.  \n",
    "- **Fewer latent factors (f=16)** are sufficient, as higher complexity leads to overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d925396",
   "metadata": {},
   "source": [
    "## 11. Model Comparison: ALS vs Item–Item vs Popularity\n",
    "\n",
    "We compare the three recommenders side by side at a fixed cutoff (**K = 10**), \n",
    "reporting **Recall@10**, **MAP@10**, and **EPR**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6978d0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>alpha</th>\n",
       "      <th>factors</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>MAP@10</th>\n",
       "      <th>EPR (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALS (linear)</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>16.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALS (linear)</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>17.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALS (linear)</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>17.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALS (linear)</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>19.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALS (linear)</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>19.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ALS (linear)</td>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>21.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ALS (log)</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ALS (log)</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0218</td>\n",
       "      <td>16.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ALS (log)</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>15.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ALS (log)</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>17.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ALS (log)</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>15.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ALS (log)</td>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>17.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Item–Item (cosine)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>15.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Popularity</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>17.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model alpha factors Recall@10  MAP@10 EPR (%)\n",
       "0         ALS (linear)    10      16    0.0088  0.0072   16.20\n",
       "1         ALS (linear)    10      32    0.0151  0.0211   17.55\n",
       "2         ALS (linear)    40      16    0.0044  0.0050   17.73\n",
       "3         ALS (linear)    40      32    0.0107  0.0082   19.61\n",
       "4         ALS (linear)   100      16    0.0019  0.0008   19.77\n",
       "5         ALS (linear)   100      32    0.0038  0.0033   21.57\n",
       "6            ALS (log)    10      16    0.0182  0.0214   15.33\n",
       "7            ALS (log)    10      32    0.0182  0.0218   16.75\n",
       "8            ALS (log)    40      16    0.0157  0.0219   15.47\n",
       "9            ALS (log)    40      32    0.0157  0.0161   17.05\n",
       "10           ALS (log)   100      16    0.0126  0.0146   15.45\n",
       "11           ALS (log)   100      32    0.0126  0.0153   17.47\n",
       "12  Item–Item (cosine)  None    None    0.0094  0.0128   15.68\n",
       "13          Popularity  None    None    0.0063  0.0056   17.48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Unified comparison at fixed K (K=10) ===\n",
    "\n",
    "def popularity_metrics(train_csr, test_csr, K=10):\n",
    "    \"\"\"Compute Recall@K, MAP@K and EPR for the global popularity baseline.\"\"\"\n",
    "\n",
    "    # Global popularity scores (same for all users)\n",
    "    item_pop = np.asarray(train_csr.sum(axis=0)).ravel()\n",
    "\n",
    "    hits, total = 0, 0\n",
    "    ap_sum, n_users_eval = 0.0, 0\n",
    "    epr_vals = []\n",
    "\n",
    "    n_items = item_pop.shape[0]\n",
    "    # Pre-sort once by popularity (desc)\n",
    "    popular_order = np.argsort(-item_pop)\n",
    "\n",
    "    for u in range(train_csr.shape[0]):\n",
    "        test_items = get_seen_items(test_csr, u)\n",
    "        if test_items.size == 0:\n",
    "            continue\n",
    "\n",
    "        seen = set(get_seen_items(train_csr, u).tolist())\n",
    "\n",
    "        # --- top-K recs: most popular not seen\n",
    "        recs = [i for i in popular_order if i not in seen][:K]\n",
    "\n",
    "        # Recall@K\n",
    "        hits += len(set(recs).intersection(set(test_items.tolist())))\n",
    "        total += len(test_items)\n",
    "\n",
    "        # MAP@K\n",
    "        ap_sum += average_precision_k(actual=test_items.tolist(), predicted=recs, k=K)\n",
    "        n_users_eval += 1\n",
    "\n",
    "        # EPR: percentile ranks under popularity scores with masking\n",
    "        # Build ranks once per user after masking seen\n",
    "        scores_u = item_pop.copy()\n",
    "        if seen:\n",
    "            scores_u[list(seen)] = -np.inf\n",
    "        order = np.argsort(-scores_u)  # descending\n",
    "        inv_rank = np.empty_like(order)\n",
    "        inv_rank[order] = np.arange(n_items)  # 0=best\n",
    "        denom = max(n_items - 1, 1)\n",
    "        for i in test_items:\n",
    "            epr_vals.append(inv_rank[i] / denom * 100.0)\n",
    "\n",
    "    recall = (round(hits / total, 4)) if total > 0 else np.nan\n",
    "    mapk = (round(ap_sum / n_users_eval, 4)) if n_users_eval > 0 else np.nan\n",
    "    epr = float((np.mean(epr_vals).round(2))) if epr_vals else np.nan\n",
    "    return recall, mapk, epr\n",
    "\n",
    "\n",
    "def item_item_metrics(S, train_csr, test_csr, K=10):\n",
    "    \"\"\"Compute Recall@K, MAP@K and EPR for item–item cosine CF given S (I x I).\"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    hits, total = 0, 0\n",
    "    ap_sum, n_users_eval = 0.0, 0\n",
    "    epr_vals = []\n",
    "\n",
    "    n_items = S.shape[0]\n",
    "\n",
    "    for u in range(train_csr.shape[0]):\n",
    "        seen = get_seen_items(train_csr, u)\n",
    "        test_items = get_seen_items(test_csr, u)\n",
    "        if test_items.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Scores by sum of similarities to seen items\n",
    "        prof = np.zeros(n_items, dtype=np.float64)\n",
    "        if seen.size > 0:\n",
    "            prof[seen] = 1.0\n",
    "        scores = prof @ S\n",
    "        if seen.size > 0:\n",
    "            scores[seen] = -np.inf\n",
    "\n",
    "        # Top-K\n",
    "        if np.isfinite(scores).any():\n",
    "            rec_idx = np.argpartition(-scores, min(K, n_items - 1))[:K]\n",
    "            recs = rec_idx[np.argsort(-scores[rec_idx])].tolist()\n",
    "        else:\n",
    "            recs = []\n",
    "\n",
    "        # Recall@K\n",
    "        hits += len(set(recs).intersection(set(test_items.tolist())))\n",
    "        total += len(test_items)\n",
    "\n",
    "        # MAP@K\n",
    "        ap_sum += average_precision_k(actual=test_items.tolist(), predicted=recs, k=K)\n",
    "        n_users_eval += 1\n",
    "\n",
    "        # EPR\n",
    "        order = np.argsort(-scores)\n",
    "        inv_rank = np.empty_like(order)\n",
    "        inv_rank[order] = np.arange(n_items)\n",
    "        denom = max(n_items - 1, 1)\n",
    "        for i in test_items:\n",
    "            epr_vals.append((inv_rank[i] / denom * 100.0))\n",
    "\n",
    "    recall = (round(hits / total, 4)) if total > 0 else np.nan\n",
    "    mapk = (round(ap_sum / n_users_eval, 4)) if n_users_eval > 0 else np.nan\n",
    "    epr = float((np.mean(epr_vals).round(2))) if epr_vals else np.nan\n",
    "    return recall, mapk, epr\n",
    "\n",
    "\n",
    "def evaluate_all_models(\n",
    "    sweep_linear_df,\n",
    "    sweep_log_df,\n",
    "    K=10,\n",
    "    S=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Build the unified comparison table at fixed K by:\n",
    "      - computing baselines (Popularity, Item–Item)\n",
    "      - concatenating precomputed ALS sweeps (linear, log)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Popularity (no hyperparams)\n",
    "    pop_recall, pop_map, pop_epr = popularity_metrics(train_matrix, test_matrix, K=K)\n",
    "    base_rows = [{\n",
    "        \"Model\": \"Popularity\",\n",
    "        \"alpha\": None, \"factors\": None,\n",
    "        f\"Recall@{K}\": pop_recall, f\"MAP@{K}\": pop_map, \"EPR (%)\": pop_epr\n",
    "    }]\n",
    "\n",
    "    # 2) Item–Item Cosine (no hyperparams)\n",
    "    if S is None:\n",
    "        S = item_item_cosine_scores(train_matrix)\n",
    "    cos_recall, cos_map, cos_epr = item_item_metrics(S, train_matrix, test_matrix, K=K)\n",
    "    \n",
    "    base_rows.append({\n",
    "        \"Model\": \"Item–Item (cosine)\",\n",
    "        \"alpha\": None, \"factors\": None,\n",
    "        f\"Recall@{K}\": cos_recall, f\"MAP@{K}\": cos_map, \"EPR (%)\": cos_epr\n",
    "    })\n",
    "\n",
    "    base_df = pd.DataFrame(base_rows)\n",
    "\n",
    "    # Ensure consistent column order\n",
    "    metric_cols = [f\"Recall@{K}\", f\"MAP@{K}\", \"EPR (%)\"]\n",
    "    ordered_cols = [\"Model\", \"alpha\", \"factors\"] + metric_cols\n",
    "    \n",
    "    # Concatenate baselines + ALS \n",
    "    all_df = pd.concat(\n",
    "        [base_df[ordered_cols], sweep_linear_df[ordered_cols], sweep_log_df[ordered_cols]],\n",
    "        ignore_index=True\n",
    "    ).sort_values([\"Model\", \"alpha\", \"factors\"]).reset_index(drop=True)\n",
    "\n",
    "    return all_df\n",
    "\n",
    "\n",
    "# === Run the unified evaluation (K fixed) ===\n",
    "K_fixed = 10\n",
    "all_models_df = evaluate_all_models(\n",
    "    sweep_linear,\n",
    "    sweep_log,\n",
    "    K=K_fixed,\n",
    "    S=S\n",
    ")\n",
    "display(all_models_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14f5bfc",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Results highlight how data sparsity strongly impacts model performance on the reduced Last.fm sample (~500 users, ~2 Million interactions).  \n",
    "\n",
    "**ALS with log confidence** achieves the best balance: it provides the lowest EPR values, meaning more stable and consistent rankings across the catalog, while also reaching the highest Recall@10 and MAP@10 among ALS variants.  \n",
    "\n",
    "**ALS with linear confidence** performs worse overall, struggling to capture useful patterns under limited data.  \n",
    "\n",
    "Interestingly, **Item–Item CF** remains competitive, with Recall and MAP comparable to ALS, confirming that neighborhood methods can still perform well when the dataset is small and sparse.  \n",
    "Finally, **Popularity** represents the weakest baseline, as expected, since it lacks personalization and is biased towards the most frequent artists.  \n",
    "\n",
    "Overall, ALS shows its advantage in terms of ranking quality (lower EPR), but Item–Item CF can still match or outperform it on top-K accuracy due to the limited scale and density of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b1286a",
   "metadata": {},
   "source": [
    "**Next Step:** \n",
    "\n",
    "rerun on a larger dataset to confirm ALS dominance across all metrics, as shown in the original paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_Science_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
